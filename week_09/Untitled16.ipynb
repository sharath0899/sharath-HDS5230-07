{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 09 Assignment - Logistic Regression Solver Comparison (PatientAnalyticFile.csv)\n",
        "# Name: Sharath Kasula"
      ],
      "metadata": {
        "id": "bIcV43WpNUa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utDT67YMNMcs"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Assignment Questions Addressed (Updated with PatientAnalyticFile.csv):\n",
        "1. Among the different classification models included in the Python notebook, which model had the best overall performance?\n",
        "Different solvers were applied in multiple logistic regression models to analyze the PatientAnalyticFile dataset. L2 regularization ran in five different optimization solvers which consisted of lbfgs, newton-cg, sag, saga and liblinear. The models were fitted with 80% of the available data while testing occurred on the hidden 20% portion of data.\n",
        "\n",
        "All models trained with various solvers demonstrated a similar training accuracy level at 66.1% and similar holdout accuracy results at 65.5% which demonstrates limited overfitting as well as generalization potential for unforeseen data. The chosen performance indicators showed no variation throughout the optimization procedures because the predictive features from the dataset created equivalent solution results across different solver types.\n",
        "\n",
        "The lbfgs solver equalled the accuracy levels of other models through its optimal runtime performance. The method provides an effective solution because short processing time or limited resources are present. The newton-cg and saga solvers took much longer to execute yet produced results with minimal accuracy advantages that negated their increased computational cost.\n",
        "\n",
        "Each fitted model failed to accurately predict patient mortality although the similar training and testing results proved the logistic regression system instituted proper regularizations which maintained stability.\n",
        "\n",
        "The generalization abilities across all solving approaches proved equivalent to each other. The lbfgs solver proved most efficient for practical usage due to its effective balance of accuracy compared to runtime requirements thus making it the best choice for logistic regression analysis on this dataset.\n",
        "2. Fit a series of logistic regression models using different solvers (with L2 regularization), using the same 80/20 train-holdout split.\n",
        "3. Report training accuracy, holdout accuracy, and time taken.\n",
        "4. Summarize findings: Which solver performed best and why?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Load and prepare the data ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "k-XvPLZuNZ4s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PatientAnalyticFile.csv\n",
        "df = pd.read_csv(\"PatientAnalyticFile.csv\")"
      ],
      "metadata": {
        "id": "tWdcIemXNh8S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create binary outcome: 1 if patient has died, 0 if alive\n",
        "df['Died'] = df['DateOfDeath'].notnull().astype(int)"
      ],
      "metadata": {
        "id": "eBFTAgqZNtLD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop non-numeric and date columns\n",
        "drop_cols = ['PatientID', 'DateOfBirth', 'Gender', 'Race',\n",
        "             'First_Appointment_Date', 'Last_Appointment_Date', 'DateOfDeath']\n",
        "X = df.drop(columns=drop_cols + ['Died'])\n",
        "y = df['Died']"
      ],
      "metadata": {
        "id": "_ce59AxTNx43"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "wPy9ilQpNzNv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data (80% train, 20% holdout)\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "GHqG10zYN4bD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Train Logistic Regression models with different solvers ---\n",
        "solvers = ['lbfgs', 'newton-cg', 'sag', 'saga', 'liblinear']\n",
        "results = []\n"
      ],
      "metadata": {
        "id": "2pO25n_mN8ev"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for solver in solvers:\n",
        "    model = LogisticRegression(penalty='l2', solver=solver, max_iter=10000)\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    holdout_acc = accuracy_score(y_holdout, model.predict(X_holdout))\n",
        "\n",
        "    results.append({\n",
        "        \"Solver\": solver,\n",
        "        \"Training Accuracy\": train_acc,\n",
        "        \"Holdout Accuracy\": holdout_acc,\n",
        "        \"Time Taken (s)\": elapsed_time\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "Ks2UCHRHN_wY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Report and display results ---\n",
        "print(\"\\nQuestion 1: Best Performing Model in Previous Notebook\")\n",
        "print(\"Answer: The Random Forest classifier had the best performance with high training and holdout accuracy.\")\n",
        "\n",
        "print(\"\\nQuestion 2 & 3: Logistic Regression Solver Comparison Table (PatientAnalyticFile.csv)\")\n",
        "print(results_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4vnphMvOD7D",
        "outputId": "23c4af04-fa12-46fb-d2e1-dfaa068609e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1: Best Performing Model in Previous Notebook\n",
            "Answer: The Random Forest classifier had the best performance with high training and holdout accuracy.\n",
            "\n",
            "Question 2 & 3: Logistic Regression Solver Comparison Table (PatientAnalyticFile.csv)\n",
            "   Solver  Training Accuracy  Holdout Accuracy  Time Taken (s)\n",
            "    lbfgs           0.660937           0.65475        0.047133\n",
            "newton-cg           0.660937           0.65475        0.125444\n",
            "      sag           0.660937           0.65475        0.761992\n",
            "     saga           0.660937           0.65475        0.381480\n",
            "liblinear           0.660937           0.65475        0.069024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Identify best solver and summarize findings ---\n",
        "best_solver = results_df.loc[results_df['Holdout Accuracy'].idxmax()]\n",
        "print(\"\\nQuestion 4: Best Solver Analysis\")\n",
        "print(f\"The best solver is '{best_solver['Solver']}' with a holdout accuracy of {best_solver['Holdout Accuracy']:.4f}.\")\n",
        "print(\"This solver provided consistent training accuracy and balanced computation time.\\n\"\n",
        "      \"All solvers performed similarly in accuracy, but the fastest solver ('lbfgs') may be preferred\\n\"\n",
        "      \"when computational efficiency is critical.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBMawLhfOH6f",
        "outputId": "dd3479ae-6d05-40c2-bd6e-9fb31d5b8acc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 4: Best Solver Analysis\n",
            "The best solver is 'lbfgs' with a holdout accuracy of 0.6548.\n",
            "This solver provided consistent training accuracy and balanced computation time.\n",
            "All solvers performed similarly in accuracy, but the fastest solver ('lbfgs') may be preferred\n",
            "when computational efficiency is critical.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Final Summary\n",
        "print(\"\\nFinal Conclusion:\")\n",
        "print(f\"Using PatientAnalyticFile.csv and evaluating 5 solvers (lbfgs, newton-cg, sag, saga, liblinear),\\n\"\n",
        "      f\"the solver '{best_solver['Solver']}' showed the best balance of generalization performance,\\n\"\n",
        "      f\"training accuracy, and execution time.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE8g-fmsOLMK",
        "outputId": "68bbdbb4-cbb9-4e2b-dde0-f5b28bf1d2cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Conclusion:\n",
            "Using PatientAnalyticFile.csv and evaluating 5 solvers (lbfgs, newton-cg, sag, saga, liblinear),\n",
            "the solver 'lbfgs' showed the best balance of generalization performance,\n",
            "training accuracy, and execution time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation used various solvers from logistic regression modeling on the PatientAnalyticFile dataset. The L2 regularization was used by lbfgs, newton-cg, sag, saga and liblinear solvers. The training process included 80% of the data with each model receiving this amount and testing took place on 20% of unprocessed data.\n",
        "\n",
        "All solvers demonstrated a training accuracy of 66.1% while holdout accuracy reached 65.5% which indicates a low level of overfitting and satisfactory generalizability to new data points. The optimization solver choice did not influence the achieved performance metrics indicating that the predictive features of the dataset delivered equal results through different solvers.\n",
        "\n",
        "The lbfgs solver produced identical accuracy outcomes as other methods while serving as the speediest execution method among all models. Its operation speed makes it favorable for cases that need limited computational power or face time constraints. Newton-cg and saga methods required extensive processing time because their execution rate was much slower yet they failed to achieve superior predictive accuracy when compared to other solvers.\n",
        "\n",
        "The consistent outcome between training and testing phases shows that logistic regression performed stably even though its accuracy remained low probably because of the enigmatic nature of patient death causation.\n",
        "\n",
        "The generalization abilities of all solvers proved equivalent to one another. From a practical perspective the lbfgs solver proved most efficient because it managed to strike the right balance between precision and execution time which made it appropriate for logistic regression work on this data set."
      ],
      "metadata": {
        "id": "tiDk0RgQVkyg"
      }
    }
  ]
}